{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings_distributed import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, CUDA_DEVICES))\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from functools import partial\n",
    "from models import attgan\n",
    "from settings_distributed import *\n",
    "from tensorflow.keras import losses, optimizers, metrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import dataloader\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=[\n",
    "    f\"/gpu:{idx}\" for idx in range(len(CUDA_DEVICES))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = dataloader.DataLoader(\"train\", BATCH_SIZE)\n",
    "valid_dloader = dataloader.DataLoader(\"valid\", BATCH_SIZE)\n",
    "test_dloader = dataloader.DataLoader(\"test\", BATCH_SIZE)\n",
    "\n",
    "with strategy.scope():\n",
    "    train_dloader = tf.data.Dataset.from_generator(\n",
    "        train_dloader.next_batch, \n",
    "        (tf.float32, tf.float32, tf.float32), \n",
    "        (\n",
    "            tf.TensorShape([None, HEIGHT, WIDTH, CHANNEL]), \n",
    "            tf.TensorShape([None, NUM_ATT]), \n",
    "            tf.TensorShape([None, NUM_ATT])\n",
    "        )\n",
    "    ).shuffle(1024)\n",
    "    train_dloader = strategy.experimental_distribute_dataset(train_dloader)\n",
    "\n",
    "    valid_dloader = tf.data.Dataset.from_generator(\n",
    "        valid_dloader.next_batch, \n",
    "        (tf.float32, tf.float32, tf.float32), \n",
    "        (\n",
    "            tf.TensorShape([None, HEIGHT, WIDTH, CHANNEL]), \n",
    "            tf.TensorShape([None, NUM_ATT]), \n",
    "            tf.TensorShape([None, NUM_ATT])\n",
    "        )\n",
    "    )\n",
    "    valid_dloader = strategy.experimental_distribute_dataset(valid_dloader)\n",
    "\n",
    "    test_dloader = tf.data.Dataset.from_generator(\n",
    "        test_dloader.next_batch, \n",
    "        (tf.float32, tf.float32, tf.float32), \n",
    "        (\n",
    "            tf.TensorShape([None, HEIGHT, WIDTH, CHANNEL]), \n",
    "            tf.TensorShape([None, NUM_ATT]), \n",
    "            tf.TensorShape([None, NUM_ATT])\n",
    "        )\n",
    "    )\n",
    "    test_dloader = strategy.experimental_distribute_dataset(test_dloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = attgan.AttGAN()\n",
    "    \n",
    "    criterion_MSE = losses.MeanSquaredError(reduction=tf.losses.Reduction.SUM)\n",
    "    criterion_BCE = losses.BinaryCrossentropy(reduction=tf.losses.Reduction.SUM)\n",
    "    \n",
    "    optimizer_gen = optimizers.Adam(learning_rate=ETA)\n",
    "    optimizer_dis = optimizers.Adam(learning_rate=ETA)\n",
    "    \n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    # utils.load_model(ckpt, \"ckpts/attgan/2020-03-07-20-12-03-Epoch12/ckpt-64\")\n",
    "\n",
    "    loss_mean_gen = metrics.Mean()\n",
    "    loss_mean_dis = metrics.Mean()\n",
    "    loss_valid = metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with strategy.scope():\n",
    "#     def criterion_MAE(y_true, y_pred):\n",
    "#         n = y_true.shape[0]\n",
    "\n",
    "#         loss = tf.math.abs(y_true - y_pred)\n",
    "#         loss = tf.reshape(loss, shape=(n, -1))\n",
    "#         # loss = tf.reduce_sum(loss, axis=-1)\n",
    "#         loss = tf.reduce_mean(loss)\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def criterion_BCE(y_true, y_pred):\n",
    "#         n = y_true.shape[0]\n",
    "\n",
    "#         loss = - y_true * tf.math.log(y_pred + 1e-6) - (1 - y_true) * tf.math.log(1 - y_pred + 1e-6)\n",
    "#         loss = tf.reshape(loss, (n, -1))\n",
    "#         # loss = tf.reduce_sum(loss, axis=1)\n",
    "#         loss = tf.reduce_mean(loss)\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def generator_loss(x, b, x_rec_a, d_rec_b, c_rec_b, training=False):\n",
    "        n = x.shape[0]\n",
    "\n",
    "        loss_rec = criterion_MSE(x, x_rec_a)\n",
    "        loss_adv_gen = criterion_BCE(tf.ones_like(d_rec_b), d_rec_b)\n",
    "        loss_att_gen = criterion_BCE(b, c_rec_b)\n",
    "\n",
    "        loss_gen = 10*loss_rec + loss_adv_gen + loss_att_gen\n",
    "\n",
    "        return loss_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def discriminator_loss(x, a, x_rec_b, d_x, d_rec_b, c_x, training=False):\n",
    "        loss_adv_dis = criterion_BCE(tf.ones_like(d_x), d_x) + criterion_BCE(tf.zeros_like(d_rec_b), d_rec_b)\n",
    "        loss_att_dis = criterion_BCE(a, c_x)\n",
    "\n",
    "        if training is True:\n",
    "            gp = utils.wgan_gp(x, x_rec_b, model.disc, num_samples=30)\n",
    "            loss_dis = loss_adv_dis + loss_att_dis + gp\n",
    "        else:\n",
    "            loss_dis = loss_adv_dis + loss_att_dis\n",
    "\n",
    "        return loss_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def forward_prop(x, a, b, training, mode):\n",
    "        n = x.shape[0]\n",
    "        \n",
    "        x_rec_a, c_rec_a, d_rec_a = model(x, a, training=training)\n",
    "        x_rec_b, c_rec_b, d_rec_b = model(x, b, training=training)\n",
    "        c_x, d_x = model.disc(x, training=training)\n",
    "\n",
    "        # print(c_x)\n",
    "\n",
    "        if mode == \"generator\":\n",
    "            loss_gen = generator_loss(x, b, x_rec_a, d_rec_b, c_rec_b, training=training)\n",
    "            return x_rec_a, x_rec_b, loss_gen\n",
    "        elif mode == \"discriminator\":\n",
    "            loss_dis = discriminator_loss(x, a, x_rec_b, d_x, d_rec_b, c_x, training=training)\n",
    "            return x_rec_a, x_rec_b, loss_dis\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def inference(x, a, b, training):\n",
    "        if training is True:\n",
    "            model.train_mode(\"generator\")\n",
    "            with tf.GradientTape() as tape:\n",
    "                x_rec_a, x_rec_b, loss_gen = forward_prop(x, a, b, training, \"generator\")\n",
    "            \n",
    "            grad_gen = tape.gradient(loss_gen, [*model.encoder.trainable_variables, *model.decoder.trainable_variables])\n",
    "            optimizer_gen.apply_gradients(zip(grad_gen, [*model.encoder.trainable_variables, *model.decoder.trainable_variables]))\n",
    "            loss_mean_gen.update_state(loss_gen)\n",
    "\n",
    "            model.train_mode(\"discriminator\")\n",
    "            \n",
    "            for i in range(5):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    x_rec_a, x_rec_b, loss_dis = forward_prop(x, a, b, training, \"discriminator\")\n",
    "\n",
    "                grad_dis = tape.gradient(loss_dis, model.disc.trainable_variables)\n",
    "                optimizer_dis.apply_gradients(zip(grad_dis, model.disc.trainable_variables))\n",
    "                loss_mean_dis.update_state(loss_dis)\n",
    "            \n",
    "        else:\n",
    "            x_rec_a, x_rec_b, loss_gen = forward_prop(x, a, b, training, \"generator\")\n",
    "            loss_valid.update_state(loss_gen)\n",
    "            \n",
    "        return x_rec_a, x_rec_b\n",
    "    \n",
    "    def concat_distributed_tensor(distributed_tensor):\n",
    "        tensor_list = []\n",
    "        \n",
    "        for device in distributed_tensor.devices:\n",
    "            tensor_list.append(distributed_tensor.get(device))\n",
    "            \n",
    "        concat_tensor = tf.concat(tensor_list, axis=0)\n",
    "        return concat_tensor\n",
    "    \n",
    "    @tf.function\n",
    "    def distributed_inference(x, a, b, training):\n",
    "        x_rec_a, x_rec_b = strategy.experimental_run_v2(inference, args=(x, a, b, training))\n",
    "        \n",
    "        x = concat_distributed_tensor(x)\n",
    "        x_rec_a = concat_distributed_tensor(x_rec_a)\n",
    "        x_rec_b = concat_distributed_tensor(x_rec_b)\n",
    "        \n",
    "        return x, x_rec_a, x_rec_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_loss(loss_mean_obj):\n",
    "    loss = loss_mean_obj.result()\n",
    "    loss_mean_obj.reset_states()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detransform_image(image):\n",
    "    image = image*256\n",
    "    image = np.clip(image, 0, 255)\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def plot_images_with_index(images, indices):\n",
    "    for i, index in enumerate(indices):\n",
    "        image = images[index].numpy()\n",
    "        image = detransform_image(image)\n",
    "        \n",
    "        plt.subplot(1, len(indices), i+1)\n",
    "        plt.imshow(image)\n",
    "\n",
    "def plot_images(x_origin, x_rec_a, x_rec_b):\n",
    "    indices = np.random.randint(x_origin.shape[0], size=3)\n",
    "    \n",
    "    print(\"Origin:\")\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plot_images_with_index(x_origin, indices)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Reconstructed with attribute a:\")\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plot_images_with_index(x_rec_a, indices)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Reconstructed with attribute b:\")\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plot_images_with_index(x_rec_b, indices)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_valid_loss = 323422.78125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "INFO:tensorflow:batch_all_reduce: 38 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 38 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    for e in range(EPOCHS):\n",
    "        print(\"===============================================================================\")\n",
    "        \n",
    "        for x_batch, att_a_batch, att_b_batch in train_dloader:\n",
    "            x, x_rec_a, x_rec_b = distributed_inference(x_batch, att_a_batch, att_b_batch, training=True)\n",
    "\n",
    "        train_loss_gen = get_mean_loss(loss_mean_gen)\n",
    "        train_loss_dis = get_mean_loss(loss_mean_dis)\n",
    "            \n",
    "        print(\"=== TRAIN SET ===\")\n",
    "        plot_images(x, x_rec_a, x_rec_b)\n",
    "\n",
    "        for x_batch, att_a_batch, att_b_batch in valid_dloader:\n",
    "            x, x_rec_a, x_rec_b = distributed_inference(x_batch, att_a_batch, att_b_batch, training=False)\n",
    "            \n",
    "        valid_loss = get_mean_loss(loss_valid)\n",
    "        \n",
    "        print(\"=== VALID SET ===\")\n",
    "        plot_images(x, x_rec_a, x_rec_b)\n",
    "        \n",
    "        print(f\"Epochs {e+1}/{EPOCHS}\")\n",
    "        print(f\"Train generator loss: {train_loss_gen:.8f}\")\n",
    "        print(f\"Train discriminator loss: {train_loss_dis:.8f}\")\n",
    "        print(f\"Valid loss: {valid_loss:.8f}\")\n",
    "\n",
    "        with open(LOG_FILE, \"a\") as logfile:\n",
    "            logfile.write(f\"Epochs {e+1}/{EPOCHS}\\n\")\n",
    "            logfile.write(f\"Train generator loss: {train_loss_gen:.8f}\\n\")\n",
    "            logfile.write(f\"Train discriminator loss: {train_loss_dis:.8f}\\n\")\n",
    "            logfile.write(f\"Valid loss: {valid_loss:.8f}\\n\")\n",
    "\n",
    "        if less_valid_loss > valid_loss:\n",
    "            less_valid_loss = valid_loss\n",
    "            utils.save_model_with_source(model, ckpt, \"ckpts/attgan\", \"models\", e+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (Tensorflow 2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
