{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, CUDA_DEVICES))\n",
    "\n",
    "LOG_FILE = \"train_log7.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from models import attgan\n",
    "from settings import *\n",
    "from tensorflow.keras import losses, optimizers, metrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import dataloader\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=[\n",
    "    f\"/gpu:{idx}\" for idx in CUDA_DEVICES\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = dataloader.DataLoader(\"train\", BATCH_SIZE)\n",
    "valid_dloader = dataloader.DataLoader(\"valid\", BATCH_SIZE)\n",
    "test_dloader = dataloader.DataLoader(\"test\", BATCH_SIZE)\n",
    "\n",
    "with strategy.scope():\n",
    "    train_dloader = tf.data.Dataset.from_generator(\n",
    "        train_dloader.next_batch, \n",
    "        (tf.float32, tf.float32), \n",
    "        (\n",
    "            tf.TensorShape([None, HEIGHT, WIDTH, CHANNEL]), \n",
    "            tf.TensorShape([None, NUM_ATT])\n",
    "        )\n",
    "    )\n",
    "    train_dloader = strategy.experimental_distribute_dataset(train_dloader)\n",
    "\n",
    "    valid_dloader = tf.data.Dataset.from_generator(\n",
    "        valid_dloader.next_batch, \n",
    "        (tf.float32, tf.float32), \n",
    "        (\n",
    "            tf.TensorShape([None, HEIGHT, WIDTH, CHANNEL]), \n",
    "            tf.TensorShape([None, NUM_ATT])\n",
    "        )\n",
    "    )\n",
    "    valid_dloader = strategy.experimental_distribute_dataset(valid_dloader)\n",
    "\n",
    "    test_dloader = tf.data.Dataset.from_generator(\n",
    "        test_dloader.next_batch, \n",
    "        (tf.float32, tf.float32), \n",
    "        (\n",
    "            tf.TensorShape([None, HEIGHT, WIDTH, CHANNEL]), \n",
    "            tf.TensorShape([None, NUM_ATT])\n",
    "        )\n",
    "    )\n",
    "    test_dloader = strategy.experimental_distribute_dataset(test_dloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = attgan.AttGAN()\n",
    "    \n",
    "    criterion_MAE = losses.MeanAbsoluteError(reduction=tf.losses.Reduction.SUM)\n",
    "    criterion_BCE = losses.BinaryCrossentropy(reduction=tf.losses.Reduction.SUM)\n",
    "    \n",
    "    optimizer = optimizers.Adam(learning_rate=ETA)\n",
    "\n",
    "    loss_mean_gen = metrics.Mean()\n",
    "    loss_mean_dis = metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def criterion_MAE(y_true, y_pred):\n",
    "#     n = y_true.shape[0]\n",
    "\n",
    "#     loss = tf.math.abs(y_true - y_pred)\n",
    "#     loss = tf.reshape(loss, shape=(n, -1))\n",
    "#     # loss = tf.reduce_sum(loss, axis=-1)\n",
    "#     loss = tf.reduce_mean(loss)\n",
    "\n",
    "#     return loss\n",
    "\n",
    "# def criterion_BCE(y_true, y_pred):\n",
    "#     n = y_true.shape[0]\n",
    "\n",
    "#     loss = - y_true * tf.math.log(y_pred + 1e-6) - (1 - y_true) * tf.math.log(1 - y_pred + 1e-6)\n",
    "#     loss = tf.reshape(loss, (n, -1))\n",
    "#     # loss = tf.reduce_sum(loss, axis=1)\n",
    "#     loss = tf.reduce_mean(loss)\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def generator_loss(x, a, b, x_rec_a, x_rec_b, d_x, d_rec_a, d_rec_b, c_x, c_rec_a, c_rec_b, training=False):\n",
    "        n = x.shape[0]\n",
    "\n",
    "        loss_rec = criterion_MAE(x, x_rec_a)\n",
    "        loss_adv_gen = tf.reduce_mean(-d_rec_b, axis=0)\n",
    "        loss_att_gen = criterion_BCE(a, c_rec_a) + criterion_BCE(b, c_rec_b)\n",
    "\n",
    "        loss_gen = loss_rec*100 + loss_adv_gen + loss_att_gen\n",
    "\n",
    "        # print(\"GENERATOR\")\n",
    "        # print(\"LOSS_GEN:\", loss_gen)\n",
    "        # print(\"REC:\", loss_rec)\n",
    "        # print(\"ADV:\", loss_adv_gen)\n",
    "        # print(\"ATT:\", loss_att_gen)\n",
    "        # print()\n",
    "\n",
    "        return loss_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def discriminator_loss(x, a, b, x_rec_a, x_rec_b, d_x, d_rec_a, d_rec_b, c_x, c_rec_a, c_rec_b, training=False):\n",
    "        loss_adv_dis = tf.reduce_mean(-d_x + d_rec_b, axis=0)\n",
    "        loss_att_dis = criterion_BCE(a, c_x)\n",
    "\n",
    "        if training is True:\n",
    "            gp = utils.wgan_gp(x, x_rec_b, model.disc)\n",
    "            loss_dis = loss_adv_dis + loss_att_dis + 10*gp\n",
    "        else:\n",
    "            loss_dis = loss_adv_dis + loss_att_dis\n",
    "\n",
    "        # print(\"DISCRIMINATOR\")\n",
    "        # print(\"LOSS_DIS:\", loss_dis)\n",
    "        # print(\"ADV:\", loss_adv_dis)\n",
    "        # print(\"ATT:\", loss_att_dis)\n",
    "        # print(\"GP:\", gp)\n",
    "        # print()\n",
    "\n",
    "        return loss_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def forward_prop(x, a, training=False):\n",
    "        n = x.shape[0]\n",
    "        num_att = a.shape[1]\n",
    "        b = utils.generate_attribute(n, num_att)\n",
    "\n",
    "        x_rec_a, c_rec_a, d_rec_a = model(x, a, training=training)\n",
    "        x_rec_b, c_rec_b, d_rec_b = model(x, b, training=training)\n",
    "        c_x, d_x = model.disc(x, training=training)\n",
    "\n",
    "        # print(c_x)\n",
    "\n",
    "        loss_gen = generator_loss(x, a, b, x_rec_a, x_rec_b, d_x, d_rec_a, d_rec_b, c_x, c_rec_a, c_rec_b, training=training)\n",
    "        loss_dis = discriminator_loss(x, a, b, x_rec_a, x_rec_b, d_x, d_rec_a, d_rec_b, c_x, c_rec_a, c_rec_b, training=training)\n",
    "\n",
    "        return x_rec_a, x_rec_b, loss_gen, loss_dis\n",
    "    \n",
    "    def inference(x, a, training):\n",
    "        if training is True:\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                x_rec_a, x_rec_b, loss_gen, loss_dis = forward_prop(x, a, True)\n",
    "            \n",
    "            grad_gen = tape.gradient(loss_gen, [*model.encoder.trainable_variables, *model.decoder.trainable_variables])\n",
    "            optimizer.apply_gradients(zip(grad_gen, [*model.encoder.trainable_variables, *model.decoder.trainable_variables]))\n",
    "\n",
    "            grad_dis = tape.gradient(loss_dis, model.disc.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grad_dis, model.disc.trainable_variables))\n",
    "            \n",
    "        else:\n",
    "            x_rec_a, x_rec_b, loss_gen, loss_dis = forward_prop(x, a, False)\n",
    "\n",
    "        loss_mean_gen.update_state(loss_gen)\n",
    "        loss_mean_dis.update_state(loss_dis)\n",
    "            \n",
    "        return x_rec_a, x_rec_b\n",
    "    \n",
    "    def concat_distributed_tensor(distributed_tensor):\n",
    "        tensor_list = []\n",
    "        \n",
    "        for device in distributed_tensor.devices:\n",
    "            tensor_list.append(distributed_tensor.get(device))\n",
    "            \n",
    "        concat_tensor = tf.concat(tensor_list, axis=0)\n",
    "        return concat_tensor\n",
    "    \n",
    "    @tf.function\n",
    "    def distributed_inference(x, a, training):\n",
    "        x_rec_a, x_rec_b = strategy.experimental_run_v2(inference, args=(x, a, training))\n",
    "        \n",
    "        x = concat_distributed_tensor(x)\n",
    "        x_rec_a = concat_distributed_tensor(x_rec_a)\n",
    "        x_rec_b = concat_distributed_tensor(x_rec_b)\n",
    "        \n",
    "        return x, x_rec_a, x_rec_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_mean_loss(loss_mean_obj):\n",
    "    loss = loss_mean_obj.result()\n",
    "    loss_mean_obj.reset_states()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detransform_image(image):\n",
    "    image = image*256 + 128\n",
    "    image = np.clip(image, 0, 255)\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def plot_images_with_index(images, indices):\n",
    "    for i, index in enumerate(indices):\n",
    "        image = images[index].numpy()\n",
    "        image = detransform_image(image)\n",
    "        \n",
    "        plt.subplot(1, len(indices), i+1)\n",
    "        plt.imshow(image)\n",
    "\n",
    "def plot_images(x_origin, x_rec_a, x_rec_b):\n",
    "    indices = np.random.randint(BATCH_SIZE, size=3)\n",
    "    \n",
    "    print(\"Origin:\")\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plot_images_with_index(x_origin, indices)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Reconstructed with attribute a:\")\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plot_images_with_index(x_rec_a, indices)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Reconstructed with attribute b:\")\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plot_images_with_index(x_rec_b, indices)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_valid_loss = 10000000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    for e in range(EPOCHS):\n",
    "        for x_batch, att_batch in train_dloader:\n",
    "            x, x_rec_a, x_rec_b = distributed_inference(x_batch, att_batch, training=True)\n",
    "\n",
    "        train_loss_gen = get_mean_loss(loss_mean_gen)\n",
    "        train_loss_dis = get_mean_loss(loss_mean_dis)\n",
    "\n",
    "        for x_batch, att_batch in valid_dloader:\n",
    "            x, x_rec_a, x_rec_b = distributed_inference(x_batch, att_batch, training=False)\n",
    "            \n",
    "        valid_loss_gen = get_mean_loss(loss_mean_gen)\n",
    "        valid_loss_dis = get_mean_loss(loss_mean_dis)\n",
    "\n",
    "        print(\"===============================================================================\")\n",
    "        print(f\"Epochs {e+1}/{EPOCHS}\")\n",
    "        print(f\"Train generator loss: {train_loss_gen:.8f}\")\n",
    "        print(f\"Train discriminator loss: {train_loss_dis:.8f}\")\n",
    "        print(f\"Valid generator loss: {valid_loss_gen:.8f}\")\n",
    "        print(f\"Valid discriminator loss: {valid_loss_dis:.8f}\")\n",
    "            \n",
    "        print(\"=== TRAIN SET ===\")\n",
    "        plot_images(x, x_rec_a, x_rec_b)\n",
    "        \n",
    "        print(\"=== VALID SET ===\")\n",
    "        plot_images(x, x_rec_a, x_rec_b)\n",
    "\n",
    "        with open(LOG_FILE, \"a\") as logfile:\n",
    "            logfile.write(f\"Epochs {e+1}/{EPOCHS}\\n\")\n",
    "            logfile.write(f\"Train generator loss: {train_loss_gen:.8f}\\n\")\n",
    "            logfile.write(f\"Train discriminator loss: {train_loss_dis:.8f}\\n\")\n",
    "            logfile.write(f\"Valid generator loss: {valid_loss_gen:.8f}\\n\")\n",
    "            logfile.write(f\"Valid discriminator loss: {valid_loss_dis:.8f}\\n\")\n",
    "\n",
    "        if less_valid_loss > valid_loss_gen:\n",
    "            less_valid_loss = valid_loss_gen\n",
    "            utils.save_model_with_source(model, \"ckpts/attgan\", \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
