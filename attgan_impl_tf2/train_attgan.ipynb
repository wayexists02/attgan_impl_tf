{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, CUDA_DEVICES))\n",
    "\n",
    "LOG_FILE = \"train_log7.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from models import attgan\n",
    "from tensorflow.keras import losses, optimizers, metrics\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import dataloader\n",
    "import utils\n",
    "\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = dataloader.DataLoader(\"train\", BATCH_SIZE)\n",
    "valid_dloader = dataloader.DataLoader(\"valid\", BATCH_SIZE)\n",
    "test_dloader = dataloader.DataLoader(\"test\", BATCH_SIZE)\n",
    "\n",
    "train_dloader = tf.data.Dataset.from_generator(\n",
    "    train_dloader.next_batch, \n",
    "    (tf.float32, tf.float32), \n",
    "    (\n",
    "        tf.TensorShape([None, HEIGHT, WIDTH, CHANNEL]), \n",
    "        tf.TensorShape([None, NUM_ATT])\n",
    "    )\n",
    ")\n",
    "\n",
    "valid_dloader = tf.data.Dataset.from_generator(\n",
    "    valid_dloader.next_batch, \n",
    "    (tf.float32, tf.float32), \n",
    "    (\n",
    "        tf.TensorShape([None, HEIGHT, WIDTH, CHANNEL]), \n",
    "        tf.TensorShape([None, NUM_ATT])\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dloader = tf.data.Dataset.from_generator(\n",
    "    test_dloader.next_batch, \n",
    "    (tf.float32, tf.float32), \n",
    "    (\n",
    "        tf.TensorShape([None, HEIGHT, WIDTH, CHANNEL]), \n",
    "        tf.TensorShape([None, NUM_ATT])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attgan.AttGAN()\n",
    "\n",
    "criterion_MAE = losses.MeanAbsoluteError(reduction=tf.losses.Reduction.SUM)\n",
    "criterion_BCE = losses.BinaryCrossentropy(reduction=tf.losses.Reduction.SUM)\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=ETA)\n",
    "\n",
    "loss_mean_gen = metrics.Mean()\n",
    "loss_mean_dis = metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def criterion_MAE(y_true, y_pred):\n",
    "#     n = y_true.shape[0]\n",
    "\n",
    "#     loss = tf.math.abs(y_true - y_pred)\n",
    "#     loss = tf.reshape(loss, shape=(n, -1))\n",
    "#     # loss = tf.reduce_sum(loss, axis=-1)\n",
    "#     loss = tf.reduce_mean(loss)\n",
    "\n",
    "#     return loss\n",
    "\n",
    "# def criterion_BCE(y_true, y_pred):\n",
    "#     n = y_true.shape[0]\n",
    "\n",
    "#     loss = - y_true * tf.math.log(y_pred + 1e-6) - (1 - y_true) * tf.math.log(1 - y_pred + 1e-6)\n",
    "#     loss = tf.reshape(loss, (n, -1))\n",
    "#     # loss = tf.reduce_sum(loss, axis=1)\n",
    "#     loss = tf.reduce_mean(loss)\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(x, a, b, x_rec_a, x_rec_b, d_x, d_rec_a, d_rec_b, c_x, c_rec_a, c_rec_b, training=False):\n",
    "    n = x.shape[0]\n",
    "\n",
    "    loss_rec = criterion_MAE(x, x_rec_a)\n",
    "    loss_adv_gen = tf.reduce_mean(-d_rec_b, axis=0)\n",
    "    loss_att_gen = criterion_BCE(a, c_rec_a) + criterion_BCE(b, c_rec_b)\n",
    "\n",
    "    loss_gen = loss_rec*100 + loss_adv_gen + loss_att_gen\n",
    "\n",
    "    # print(\"GENERATOR\")\n",
    "    # print(\"LOSS_GEN:\", loss_gen)\n",
    "    # print(\"REC:\", loss_rec)\n",
    "    # print(\"ADV:\", loss_adv_gen)\n",
    "    # print(\"ATT:\", loss_att_gen)\n",
    "    # print()\n",
    "\n",
    "    return loss_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(x, a, b, x_rec_a, x_rec_b, d_x, d_rec_a, d_rec_b, c_x, c_rec_a, c_rec_b, training=False):\n",
    "    loss_adv_dis = tf.reduce_mean(-d_x + d_rec_b, axis=0)\n",
    "    loss_att_dis = criterion_BCE(a, c_x)\n",
    "\n",
    "    if training is True:\n",
    "        gp = utils.wgan_gp(x, x_rec_b, model.disc)\n",
    "        loss_dis = loss_adv_dis + loss_att_dis + 10*gp\n",
    "    else:\n",
    "        loss_dis = loss_adv_dis + loss_att_dis\n",
    "\n",
    "    # print(\"DISCRIMINATOR\")\n",
    "    # print(\"LOSS_DIS:\", loss_dis)\n",
    "    # print(\"ADV:\", loss_adv_dis)\n",
    "    # print(\"ATT:\", loss_att_dis)\n",
    "    # print(\"GP:\", gp)\n",
    "    # print()\n",
    "\n",
    "    return loss_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, a, training=False):\n",
    "    n = x.shape[0]\n",
    "    num_att = a.shape[1]\n",
    "    b = utils.generate_attribute(n, num_att)\n",
    "\n",
    "    x_rec_a, c_rec_a, d_rec_a = model(x, a, training=training)\n",
    "    x_rec_b, c_rec_b, d_rec_b = model(x, b, training=training)\n",
    "    c_x, d_x = model.disc(x, training=training)\n",
    "\n",
    "    # print(c_x)\n",
    "\n",
    "    loss_gen = generator_loss(x, a, b, x_rec_a, x_rec_b, d_x, d_rec_a, d_rec_b, c_x, c_rec_a, c_rec_b, training=training)\n",
    "    loss_dis = discriminator_loss(x, a, b, x_rec_a, x_rec_b, d_x, d_rec_a, d_rec_b, c_x, c_rec_a, c_rec_b, training=training)\n",
    "\n",
    "    return x_rec_a, x_rec_b, loss_gen, loss_dis\n",
    "\n",
    "@tf.function\n",
    "def inference(x, a, training):\n",
    "    if training is True:\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            x_rec_a, x_rec_b, loss_gen, loss_dis = forward_prop(x, a, True)\n",
    "\n",
    "        grad_gen = tape.gradient(loss_gen, [*model.encoder.trainable_variables, *model.decoder.trainable_variables])\n",
    "        optimizer.apply_gradients(zip(grad_gen, [*model.encoder.trainable_variables, *model.decoder.trainable_variables]))\n",
    "\n",
    "        grad_dis = tape.gradient(loss_dis, model.disc.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_dis, model.disc.trainable_variables))\n",
    "\n",
    "    else:\n",
    "        x_rec_a, x_rec_b, loss_gen, loss_dis = forward_prop(x, a, False)\n",
    "\n",
    "    loss_mean_gen.update_states(loss_gen)\n",
    "    loss_mean_dis.update_states(loss_dis)\n",
    "\n",
    "    return x_rec_a, x_rec_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_mean_loss(loss_mean_obj):\n",
    "    loss = loss_mean_obj.result()\n",
    "    loss_mean_obj.reset_states()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_valid_loss = 10000000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    for e in range(EPOCHS):\n",
    "\n",
    "        for x_batch, att_batch in train_dloader:\n",
    "            x_rec_a, x_rec_b = inference(x_batch, att_batch, training=True)\n",
    "\n",
    "        train_loss_gen = get_mean_loss(loss_mean_gen)\n",
    "        train_loss_dis = get_mean_loss(loss_mean_dis)\n",
    "\n",
    "        for x_batch, att_batch in valid_dloader:\n",
    "            x_rec_a, x_rec_b = inference(x_batch, att_batch, training=True)\n",
    "\n",
    "        valid_loss_gen = get_mean_loss(loss_mean_gen)\n",
    "        valid_loss_dis = get_mean_loss(loss_mean_dis)\n",
    "\n",
    "        print(f\"Epochs {e+1}/{EPOCHS}\")\n",
    "        print(f\"Train generator loss: {train_loss_gen:.8f}\")\n",
    "        print(f\"Train discriminator loss: {train_loss_dis:.8f}\")\n",
    "        print(f\"Valid generator loss: {valid_loss_gen:.8f}\")\n",
    "        print(f\"Valid discriminator loss: {valid_loss_dis:.8f}\")\n",
    "\n",
    "        with open(LOG_FILE, \"a\") as logfile:\n",
    "            logfile.write(f\"Epochs {e+1}/{EPOCHS}\\n\")\n",
    "            logfile.write(f\"Train generator loss: {train_loss_gen:.8f}\\n\")\n",
    "            logfile.write(f\"Train discriminator loss: {train_loss_dis:.8f}\\n\")\n",
    "            logfile.write(f\"Valid generator loss: {valid_loss_gen:.8f}\\n\")\n",
    "            logfile.write(f\"Valid discriminator loss: {valid_loss_dis:.8f}\\n\")\n",
    "\n",
    "        if less_valid_loss > valid_loss_gen:\n",
    "            less_valid_loss = valid_loss_gen\n",
    "            utils.save_model_with_source(model, \"ckpts/attgan\", \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
